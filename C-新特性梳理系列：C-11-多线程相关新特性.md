---
title: C++新特性梳理系列：C++11 多线程相关新特性
date: 2021-05-20 21:48:20
tags:
- C++11
- 多线程
---

## 前言

> 本文不会对多线程的概念进行介绍，仅对C++11新增的多线程相关的工具和库进行介绍

在C++11之前，要编写多线程程序的话，一般都会使用pthread，例如创建线程用pthread_create，互斥量则使用pthread_mutex_xx系列函数，条件变量使用pthread_cond_xx系列函数等等，严格意义上来说，这不算是C++语言上支持的东西。而是Linux系统提供的，更准确的说是Linux实现了**POSIX线程标准**描述的API，这些API都是pthread_开头。作为对比，下面是简单看一个使用pthread库的示例，如果对这些API比较了解的话，可以跳过这部分。

```c++
#include <iostream>
#include <pthread.h>

using namespace std;

int g_count = 0;
pthread_mutex_t g_mutex;

void* addCount(void* arg) {
    for (int i = 0; i < 1000000; i++) {
        //lock
        pthread_mutex_lock(&g_mutex);
        g_count++;
        //unlock
        pthread_mutex_unlock(&g_mutex);
    }
    return NULL;
}

int main()
{
    //初始化mutex
    pthread_mutex_init(&g_mutex, NULL);

    //创建两个线程
    pthread_t th1, th2;
    pthread_create(&th1, NULL, addCount, NULL);
    pthread_create(&th2, NULL, addCount, NULL);
    
    //阻塞主线程，直到两个线程执行结束
    pthread_join(th1, NULL);
    pthread_join(th2, NULL);

    std::cout << "result count : " << g_count << std::endl;
}
```

这个示例就是多线程编程中的“Hello World”，很简单，不多解释了。可以看到这个示例给人的感觉很“古老”，不够“新”（但这不代表代码不好），感觉就是在写C语言代码，确实，这段代码移植到C语言中几乎没有任何问题（如果移植到其他操作系统平台可能会比较麻烦，因为并不是所有操作系统都实现了POSIX线程标准API，比如windows）。等后面介绍C++11中多线程的新特性之后，再重写一下这段代码，使其更加的“现代化”，那么现在开始正式介绍C++11中新增的多线程相关特性。                                                     

## C++11中的多线程

C++11终于对多线程在语言层面上提供了支持，新增了5个头文件，分别是：

- `<atomic>`，包含了对原子类型的支持。
- `<thread>`，主要是std::thread线程类，该类提供几个方便的函数，例如join，detach等。
- `<mutex>`，包含了互斥量相关的类，包含来常用的lock_guard（作用类似android上封装的AUTO LOCK）。
- `<condition_variable>`，包含了条件变量相关的类。
- `<future>`，包含了对future模式的支持，即异步编程的支持。

使用这些头文件提供的类或者函数都是跨平台，在不同的平台上，会使用不同的底层实现，例如在Linux下就使用pthread那套，在windows下就使用windows那套，这是这套多线程工具库的优点之一。除此之外，这些类或者函数都比直接使用平台的API更简单方便，接下来将逐个对这些头文件中常用的一些类或者函数作介绍。

### thread

首先就是`<thread>`头文件，该头文件包含的东西不多，主要是std::thread类以及几个实用的函数，例如get_id，sleep_for（包含在this_thread namespace里）等。

#### 创建线程

创建线程的方法很简单，创建线程对象即可。std::thread有3个可用的构造函数：

```c++
thread() noexcept;   //构造一个空的线程对象

thread( thread&& other ) noexcept;  //移动构造函数

template< class Function, class... Args >
explicit thread( Function&& f, Args&&... args );   //接受传入一个可调用对象，以及其参数
```

其拷贝构造是被显式设置为delete的，所以thread对象不支持拷贝。特别的，空的线程对象并不能代表一个线程，请注意。

下面是一个简单的示例：

```c++
#include <thread>

void doBoringThing()
{
    for (int i = 0; i < 10; i++) {
        //this_thread::get_id() 获取当前线程的线程id
        std::cout << "Thread-" << this_thread::get_id() << "print: " << i << std::endl;
    }
}

int main()
{
    std::thread th1(doBoringThing);
    std::thread th2(doBoringThing);
    th1.join();
    th2.join();
    std::cout << "result count : " << g_count << std::endl;
}
```

比使用pthread_create简单一些吧，而且对线程执行函数的签名（pthread要求返回值和参数都是void*类型）没有了严格要求，这得益于模板以及可变模板参数特性。

std::thread类有几个常用的函数也值得说一下：

- join，上面示例用到了，和pthread_join作用一样，阻塞当前线程，等待目标线程返回。示例中是阻塞主线程。
- get_id()，和示例用的this_thread::get_id()不同，std::thread::get_id()拿到的是本线程对象的线程id，只要能拿到线程对象，无论现在是否是该线程执行，都可以拿到对应的id。
- detach，分离线程，和pthread_detach作用一样，简单来说就是脱离线程对象的控制，线程结束了马上就释放资源。
- swap，交换两个线程对象。
- joinable，线程是有几个状态的，joinable就是其中一个，这个函数用来判断线程的状态是否是joinable。

其他函数比如native_handle，hardware_concurrency使用不同，有兴趣的话可以自行查阅文档（我也是看的文档....）。this_thread下也还有几个函数，也一样，这种东西看文档一下就知道了，有兴趣查阅文档吧。

#### mutex

##### 常用的几个互斥量

`<mutex>`头文件中的主要内容是互斥量，互斥量是在多线程同步是使用最广泛的工具，通过对互斥量加锁和解锁来控制多线程同步。C++11中几个常用的互斥量有：

- std::mutex，最简单朴素的互斥量，具有完整的互斥语义。
- std::recursive_mutex，可递归的互斥量，允许同一个线程多次访问，可以降低死锁的风险。
- std::timed_mutex，作用同std::mutex，提供了超时的功能，可结合std::try_lock_for和std::try_lock_until使用。
- std::recursive_timed_mutex，作用同std::recursive_mutex，同样提供了超时功能。

上述几个互斥量都提供了lock,unlock,try_lock函数，从名字就可以看出来其作用，不多解释了。现在可以重写本文一开始的那个累加的程序了：

```c++
#include <thread>
#include <mutex>

void addCount(std::mutex& m, int& count) {
    for (int i = 0; i < 100000; i++) {
        m.lock();
        count++;
        m.unlock();
    }
}

int main()
{
    std::mutex m;
    int count = 0;
    //注意一下这里传递addCount的参数时，由于addCount的两个参数是引用类型，而模板参数无法推导引用类型，所以这里我们需要使用std::ref来返回一个	 std::reference_wrapper，进而辅助模板参数决定是否该使用引用传递
    std::thread t1(addCount, std::ref(m), std::ref(count));
    std::thread t2(addCount, std::ref(m), std::ref(count));

    t1.join();
    t2.join();

    std::cout << "result: " << count << std::endl;
}
```

现在这个程序看起来是不是更像C++了，更“现代”一些了（当然，程序太过简单，也许没有太大感受）。下面再看一下recursive_mutex的使用：

```c++
void boringFunc1(std::recursive_mutex& rm)
{
    rm.lock();
    std::cout << "Thread-" << std::this_thread::get_id() << " access boringFunc1" << std::endl;
    rm.unlock();
}

void boringFunc2(std::recursive_mutex& rm)
{
    rm.lock();
    std::cout << "Thread-" << std::this_thread::get_id() << " access boringFunc2" << std::endl;
    boringFunc1(rm);
    std::cout << "Thread-" << std::this_thread::get_id() << " back access boringFunc2" << std::endl;
    rm.unlock();
}

int main()
{
    std::recursive_mutex rm;
    std::thread t1(boringFunc1, std::ref(rm));
    std::thread t2(boringFunc2, std::ref(rm));

    t1.join();
    t2.join();
}
```

这里boringFunc2会去访问boringFunc1，然后继续对互斥量进行lock，由于这是递归的互斥量，所以这里是可以上锁成功的。如果是普通的std::mutex，则这里肯定会死锁。实际上，递归锁也可以叫做可重入锁，因为可重入的语义打破了死锁的一个必要条件：不可重入。所以使用std::recursive_mutex是避免死锁的一种方法。

另外两个是具备超时机制的，需要结合try_lock来使用，这个机制可以防止长时间阻塞从而降低效率，大多数使用场景就是轮询以及异步，如果各位有兴趣的话，可以自行尝试写写demo，这里就不多介绍了。接下来继续看其他部分内容。

##### 两个互斥量的wapper

下面是两个对互斥量的wapper：	

- std::lock_guard，这是一个模板类，通过一个互斥量来初始化lock_guard对象。其作用就是构造对象的时候lock，在出作用域并且析构的时候unlock，和android上的auto_lock作用差不多。

- unique_lock，也是一个模板类，其具备lock_guard的所有功能，并且在此基础上提供了延迟加锁的功能，但后续需要手动lock，同时后面将会介绍到的条件变量也必须使用unique_lock来包装。unique_lock不能复制，这也是unique的意义，unique_lock构造时可以接受第二个参数，表示对象构造时对互斥量要采取的行为，分别是：

  - std::try_lock_t，表示尝试去加锁，如果锁的状态是不可用的，也会立即返回，不会阻塞线程。不过使用这个东西也有个坑，如果使用的互斥量是std::mutex，并且线程已经获取到mutex对象，即加锁成功，那再次执行try_lock将是未定义的行为，请小心使用。
  - std::defer_lock_t，表示延迟加锁，后续是否要lock都需要用户自行决定。
  - std::adopt_lock_t，表示假设绑定的这个mutex已经加过锁了。std::lock_guard可以也只能使用这个选项。

  无论使用上述哪个选项，如果对应的mutex已经加锁了，那么当unique_lock析构的时候就要解锁。

使用std::lock_guard和unique_lock对之前的累加程序作小小修改，示例如下：

```c++
//lock_guard
void addCount(std::mutex& m, int& count) {
    for (int i = 0; i < 100000; i++) {
        std::lock_guard<std::mutex> lock(m);
        count++;
    }
}
```

很简单对吧，使用lock_guard包装mutex，就不需要关心unlock了。当然，如果需要更加细致的控制，完全可以开一个新的作用域，{}包裹起来即可。因为unique_lock包含了lock_guard的所有功能，所以上面改成使用unique_lock也是没有问题的，只是可能没必要。下面一个例子展示了unique_lock独有的功能：

```c++
void deferLockTest(std::mutex& m, int& count) {
    auto start = chrono::system_clock::now();
    for (int i = 0; i < 1000000; i++) {
        std::unique_lock<std::mutex> lock(m, std::defer_lock);
        lock.lock(); //需要手动lock，使用std::defer_lock参数的话，构造对象时实际并没有加锁，各位可以验证一下
        count++;
    }
    auto end = chrono::system_clock::now();
    std::cout << "deferLockTest run time: " << chrono::duration_cast<chrono::milliseconds>(end - start).count() << "ms." << std::endl;
}

void adoptLockTest(std::mutex& m, int& count) {
    auto start = chrono::system_clock::now();
    for (int i = 0; i < 1000000; i++) {
        m.lock(); //需要手动在构造unique_lock对象之前lock，使用std::adopt_lock参数的话，构造对象时实际并没有加锁，各位可以验证一下
        std::unique_lock<std::mutex> lock(m, std::adopt_lock);
        count++;
    }
    auto end = chrono::system_clock::now();
    std::cout << "adoptLockTest run time: " << chrono::duration_cast<chrono::milliseconds>(end - start).count() << "ms." << std::endl;
}

void tryToLockTest(std::mutex& m, int& count)
{
    auto start = chrono::system_clock::now();
    int i = 0;
    while (i < 1000000) {
        std::unique_lock<std::mutex> lock(m, std::try_to_lock);
        //使用owsn_lock()来判断是否已经加锁成功，这里加锁成功则count+1
        if (lock.owns_lock()) {
            count++;
            i++;
        }
    }
    auto end = chrono::system_clock::now();
    std::cout << "tryToLockTest run time: " << chrono::duration_cast<chrono::milliseconds>(end - start).count() << "ms." << std::endl;
}

void uniqueLockTest()
{
    std::mutex m1, m2, m3;
    int count1 = 0;
    int count2 = 0;
    int count3 = 0;
    std::thread t1(deferLockTest, std::ref(m1), std::ref(count1));
    std::thread t2(deferLockTest, std::ref(m1), std::ref(count1));
    t1.join();
    t2.join();

    std::thread t3(adoptLockTest, std::ref(m2), std::ref(count2));
    std::thread t4(adoptLockTest, std::ref(m2), std::ref(count2));
    t3.join();
    t4.join();

    std::thread t5(tryToLockTest, std::ref(m3), std::ref(count3));
    std::thread t6(tryToLockTest, std::ref(m3), std::ref(count3));
    t5.join();
    t6.join();

    std::cout << "uniqueLockTest - result count1 : " << count1 << std::endl;
    std::cout << "uniqueLockTest - result count2 : " << count2 << std::endl;
    std::cout << "uniqueLockTest - result count3 : " << count3 << std::endl;

}
```

为了对比三者的效率，示例中添加了计时。根据我本地测试，针对此示例程序，try_to_lock比其他两位的性能差一些。这可能是存在多次轮询的原因，其他场景并不就一定差。下面是我的本地测试结果：

```c++
deferLockTest run time: 235ms.
deferLockTest run time: 236ms.
adoptLockTest run time: 183ms.
adoptLockTest run time: 186ms.
tryToLockTest run time: 397ms.
tryToLockTest run time: 398ms.
uniqueLockTest - result count1 : 2000000
uniqueLockTest - result count2 : 2000000
uniqueLockTest - result count3 : 2000000
```

> 篇幅原因，我只贴出1此测试结果，应该测试多次取平均值才有比较合理的结论。

接下来看下mutex头文件中的两个有用的工具函数。

##### 两个有用的函数

还有2个有用的函数：

- std::try_lock，尝试同时对多个互斥量加锁。当某个互斥量的状态为不可用（也许是被其他线程占有了，也许是销毁了）时，会返回从0开始的索引，这个索引是表示是哪个锁开始失败的，因为这个函数接受至少两个互斥量（1个的话，没必要用这个，用mutex自带的成员变量try_lock即可）。同时如果中间某个互斥量加锁失败，则会对之前加锁成功的互斥量解锁，并立即返回失败的索引。这个也是异常安全的，当发生异常时，已经加锁的互斥量也会被解锁。
- std::lock，同时对多个互斥量加锁，是阻塞的，直到所有的互斥量加锁成功才退出阻塞状态。使用这个函数可以有效避免很多可能发生死锁的场景，因为他保证了加锁顺序，破坏了循环等待的条件。稍后会写一个简单的例子来说明。

下面是一个最简单的死锁示例：

```c++
void nothingFunc(std::mutex& m1, std::mutex& m2)
{

    std::lock_guard<std::mutex> lock1(m1);
    //sleep 200ms，为了好复现死锁
    //Ps：这里很有趣，使用了本系列之前的文章中提到过的“用户自定义字面值常量”，由于这是std里的，所以可以不以下划线开头，从这里可以看到字面常量很是很有用的
    std::this_thread::sleep_for(200ms);
    std::lock_guard<std::mutex> lock2(m2);

    std::cout << "Thread-" << std::this_thread::get_id() << " run nothingFun" << std::endl;
}

void lockTest()
{
    std::mutex m1, m2;
    //注意这里传递参数的顺序
    std::thread t1(nothingFunc, std::ref(m1), std::ref(m2));
    std::thread t2(nothingFunc, std::ref(m2), std::ref(m1));

    t1.join();
    t2.join();
}
```

这个是个会发生死锁的程序（不是100%），不多解释为何会死锁了，相信各位很容易理解。要解决这个问题也很简单，即从源头出发，把17和18行的两个参数m1,m2的顺序调整成一样就行。本例还比较容易发现错误，但如果逻辑负责，这种错误是很难发现的，根据经验，这至少要经历""查log->加log复现->发现死锁->加log复现->....(中间可能有多次无效的加log)->定位的锁顺序错了->解决问题"这样一个过程，很费时费力。幸运的是，使用std::lock可以有效防止由于锁顺序导致的死锁问题，如下(仅修改nothingFunc)：

```c++
void nothingFunc(std::mutex& m1, std::mutex& m2)
{
    std::lock(m1, m2);
    std::lock_guard<std::mutex> lock1(m1, std::adopt_lock);
    std::lock_guard<std::mutex> lock2(m2, std::adopt_lock);

    std::cout << "Thread-" << std::this_thread::get_id() << " run nothingFun" << std::endl;
}
```

对于本例，这样修改就肯定不会发生死锁了，因为std::lock直到对m1和m2（不一定是顺序）都加锁成功后才会从阻塞状态返回。这样就不存在t1拿着m1，等m2，t2拿着m2，等m1这样循环等待的情况，破坏了死锁发生的必要条件。

使用try_lock也可以解决本例的死锁问题，但需要多写个判断来判断是否对两个mutex都加锁成功。try_lock相对lock更加灵活，使用得当的话，性能会比闭锁的lock效率高。限于篇幅（其实是偷懒），我也不再写示例程序，建议各位尝试一下try_lock。

##### std::call_once

`<mutex>`里还有一个有趣的函数叫做call_once，之所以单独抽出来介绍，是因为要使用这个功能，还需要使用另一个与之配套的类std::once_flag。call_once和std::once_flag组合使用可以实现这样的效果：使某个任务（可以是函数或者其他可调用对象）在多线程环境下**保证**只执行一次！常常用于多线程环境下的初始化场景。

下面拿单例模式举例，利用call_once可以实现线程安全的单例：

```c++
class Singleton
{
public:
    static std::shared_ptr<Singleton> getInstance()
    {
        std::call_once(m_instance_created_flag,[](){
            std::cout << "call once" << std::endl;
            m_instance.reset(new Singleton());
        });
        return m_instance;
    }
private:
    Singleton() = default;
    Singleton(const Singleton&) = delete;
    Singleton& operator=(const Singleton&) = delete;
   
    static std::shared_ptr<Singleton> m_instance;
    static std::once_flag m_instance_created_flag;
};

std::once_flag Singleton::m_instance_created_flag;
std::shared_ptr<Singleton> Singleton::m_instance;

void callOnceTest()
{
    std::thread t1(&Singleton::getInstance);
    std::thread t2(&Singleton::getInstance);
    std::thread t3(&Singleton::getInstance);
    std::thread t4(&Singleton::getInstance);

    t1.join();
    t2.join();
    t3.join();
    t4.join();
}
```

运行后可以发现在多线程环境下"call once"只打印了一次，即只构造了一次SIngleton对象，是符合单例模式的。

> Ps: 实现线程安全的单例模式还有更简单的方法，即利用局部static只会在第一次执行时初始化的原理，这样的单例更简单。

> PPs: 细心的朋友可能会注意到，这里的单例是用shared_ptr包装的，这是因为如果使用裸指针的话，即使是单例，也会有裸指针的那些问题。既然用起了C++11，就尽量避免使用裸指针吧。另外，这里不一定要使用shared_ptr，使用unique_prt也是可以的，但要注意的是，不能在函数中返回unique_ptr，因为unique_ptr是不能被拷贝的，可以返回他的引用，例如：
>
> ```c++
> static Singleton& getInstance()
> {
>     std::call_once(m_instance_created_flag,[](){
>         std::cout << "call once" << std::endl;
>         m_instance.reset(new Singleton());
>     });
>     return *m_instance;
> }
> 
> static std::unique_ptr<Singleton> m_instance;
> ```

#### condition_variable

条件变量也是一种线程同步的手段，适合用在符合生产者消费者模型的场景，消费者需要等待生产者完成任务才去执行消费者自己的逻辑。如果仅使用互斥量，也可以实现这个目的，即使用超时轮询的方式，但这样会导致一些性能浪费，甚至极限场景下某些线程会一直无法加锁成功。而条件变量是基于事件通知的，当某个条件达成的时候才会唤醒其他线程，继续执行安排好的任务。这种方式既没有让CPU空转，其工作方式也很好理解，会使用之后基本不会出现太大问题。

`<condition_variable>`头文件提供了一些类和函数来支持使用条件变量：

- condition_variable类，需要结合std::unique_lock来使用。
- condition_variable_any类，并不强制要求使用std::unique_lock，实际中很少看到有使用。
- cv_status枚举类，这是一个scoped enum，用来表示条件变量是否是因为超时导致的从阻塞状态醒来。只有timeout和no_timeout两个枚举量。
- notify_all_at_thread_exit函数，当线程退出时，唤醒所有在这个条件变量上等待的线程。

我们重点介绍condition_variable类，其他的在示例程序中用到的话会再简单介绍。下面是一个示例：

```c++
//生产者
void produce(std::condition_variable& cv, std::mutex& m, std::queue<int>& q, bool& isStop)
{
    for (int i = 0; i < 10; i++) {
        //注意这里是单独的一个作用域，因为在notify之前，需要先unlock，防止出现另一个线程起来了，无法lock的情况
        {
            std::unique_lock<std::mutex> lock(m);
            q.push(i);
        	std::cout << "produce : " << i << std::endl;
        }
        cv.notify_one();
    }
	//这里是标记生产结束了，同样，注意unlock的时机
    {
        std::unique_lock<std::mutex> lock(m);
        isStop = true;
    }
    //这里使用notify_all很重要，在文中细说
    cv.notify_all();
}

//消费者
void consume(std::condition_variable& cv, std::mutex& m, std::queue<int>& q, bool& isStop)
{
    while (true) {
        int val = -1;
        {
            std::unique_lock<std::mutex> lock(m);
            //wait的第二个参数是一个函数对象，这里用lambda来写了。线程只有在被唤醒且该条件成立的情况下才会真正从阻塞状态返回
            cv.wait(lock, [&](){
                //这个顺序也要注意一下，||是短路的
                return isStop || !q.empty();
            });
            std::cout << "Thread- " << std::this_thread::get_id() << "wakeup!!" << std::endl;
            //判断退出条件
            if (isStop && q.empty()) return;
            val = q.front();
            q.pop();
        	std::cout << "Thread- " << std::this_thread::get_id() << " consume " << val << std::endl;
        }

    }
}

void conditionVariableTest()
{
    std::mutex m;
    std::condition_variable cv;
    std::queue<int> q;
    bool isStop;
    std::thread t1(produce, std::ref(cv), std::ref(m), std::ref(q), std::ref(isStop));
    std::thread t2(consume, std::ref(cv), std::ref(m), std::ref(q), std::ref(isStop));
    std::thread t3(consume, std::ref(cv), std::ref(m), std::ref(q), std::ref(isStop));

    t1.join();
    t2.join();
    t3.join();
}
```

对于示例的一部分说明已经在注释中写了。这里主要说一下produce函数最后的那个notify_all调用，各位看代码的时候可能会疑惑这里为什么是notify_all？能不能用notify_one呢？先说一下两者的区别：

- notify_one唤醒某一个在wait的线程，该线程被唤醒后会马上去尝试获得锁，所以不存在锁竞争。
- notify_all唤醒所有在wait的线程，会发生锁竞争。剩下没有拿到锁的线程会继续阻塞，**不过这次阻塞不是阻塞在条件变量上，而是在互斥量上，这很重要。**

基于上述描述，得到如下结论：

- 线程要从wait状态返回是需要满足两个条件的：**被notify通知到以及能成功对互斥量重新加锁。**注意，这个结论非常重要。

基于结论，存在如下两种情况：

1. 线程收到了notify，并且条件判断ok，然后去尝试获取互斥量，发现获取不到，那么该线程会继续在这个互斥量上阻塞，等待其他线程释放互斥锁。
2. 线程收到了notify，然后去加锁，并且加锁成功，那么会直接从wait状态中返回。

为了保证所有的线程都能正常消费完毕且退出，需要使用notify_all来向所有线程发送信号，之后他们只需要去争夺锁资源就可以了，而争夺锁资源这个事与条件变量并没有必然的联系，不会收到条件变量的影响。所以最终所有线程都能正常退出。

条件变量是一个有力的工具，但还需要注意要搭配判断状态的条件来使用，即wait之后的那个条件。否则可能会出现线程错过了notify的情况，即**notify在前，wait之后的情况。**

> 抛一个问题留给各位，各位可以尝试使用notify_one替换notify_all，然后多开几个线程，多执行几次程序。看看会有什么问题，问题的原因又是什么。

## future

`<future>`这个头文件提供了支持用户完成future/promise模式异步编程的一系列类和函数。所谓的异步，是相对于同步来说的，同步就是要执行一个任务，必须等待该任务的结果返回，当前线程才能继续执行剩下的逻辑，而异步则将任务丢给另一个线程去执行，当前线程无需等待任务结果返回。注意不要混淆这里所说的**同步**和上面提到的使用mutex，condition等线程同步手段中的**同步**，线程同步那是多线程环境下保护共享变量的手段，而这里所说的同步指的是一种编程模型。

> 关于异步编程更加详细的介绍，各位可以自行查阅资料，网上有很多。

下面是`<future>`头文件提供的一些常用的类和函数：

- promise，保存值或者异常，一般用于保存任务的结果，需要和future关联使用。每个promise都有一个共享状态，用于表示promise的结果是个什么状态，这个共享状态和future关联，这就是共享的含义，状态有如下3种：
  - make ready，即promise已经存储了结果或者异常，其他阻塞在与promise关联的future的线程会被唤醒。
  - release，promise释放了共享状态。
  - abandon，promise存储了std::future_errc::broken_promise这个值。abandon状态之后会成为make ready状态，然后将其释放，即release。
- future和shared_future，用来获取异步任务的结果的对象，这里先可以简单理解为一个句柄（不严谨，仅为了说明其作用），可以直接调用构造函数构造，但没有什么作用的。一般都是通过promise，async, packaged_task来创建，并与之关联。shared_future相对于future来说，多了可以拷贝的能力，多个shared_future可以共享同一个共享状态，并且都可以获得其对应的值。
- packaged_task，将可调用对象打包在一起成为一个对象，并且可以异步的获取对应的可调用对象返回的结果，说白了，就是std::function + 异步。后续使用到的时候就可以理解了。
- async，很直白，以异步的方式执行任务，调用后返回一个future，用于后续获取任务的结果。使用async比使用promise+future更加简单，也更加直观。调用std::async的时候可以指定其创建线程的策略，如果不指定策略则系统会自行决定是否需要立刻创建一个线程。至于有哪些策略，各位可自行查阅文档，不再赘述。

下面是使用示例：

```c++
long calcSum(int start, int end)
{
    long res = 0;
    for (int i = start; i <= end; i++) {
        res += i; 
    }
    return res;
}

void sendResult(long result, std::string message)
{
    if (result == 0) {
        std::cout << message << std::endl;
    } else {
        std::cout << message << result << std::endl;
    }
}

void asyncTest()
{
    //使用packaged_task封装函数，这个task可以返回一个future，用于后续获取结果
    std::packaged_task<long(void)> task(std::bind(calcSum, 1, 1000));
    std::future<long> f1 = task.get_future();
    //注意，packged_task的拷贝构造是被设置为delete的，所以这里传递，只能使用std::move
    std::thread t1(std::move(task));
    
    std::future<long> f2 = std::async(std::launch::async, calcSum, 1001, 2000);
    std::future<long> f3 = std::async(std::launch::async, calcSum, 2001, 3000);
    std::future<long> f4 = std::async(std::launch::async, calcSum, 3001, 4000);

    int sleepCount = 4;
    long result = 0;
    while (sleepCount > 0) {
        std::cout << "main thread still do something..." << std::endl;
        std::this_thread::sleep_for(500ms);
        //这里可以先发送一个初步结果给需要的用户，然后主线程也还可以接受其他计算请求
        sendResult(result, "result is still calculating, please wait....");
        sleepCount--;
    }

    result += f1.get();
    result += f2.get();
    result += f3.get();
    result += f4.get();
    sendResult(result, "async calc sum (1~4000) == ");
}
```

这个示例很简单，就是将1～4000的累加拆分成4个任务，然后用4个线程去作计算，最后去获取结果。在创建线程到最后去获取结果的过程中，主线程是不会阻塞的，可以继续执行后面的逻辑。这时候，各位可能会有疑问了，这里直接用4个普通线程去做，主线程也可以不阻塞啊，那这个异步的好处在哪？

首先明确，多线程和所谓的同步或者异步是两个不同维度的概念，多线程是可以实现异步的一种手段，多线程和异步并不是两种不同的编程模型。所以这里使用4个普通线程去作这个事，是完全可以的，那其实也是实现了异步模型，两者并不冲突。这里使用C++11提供的新API只是可以更方便快捷的编写异步程序，例如可以通过future方便的获取返回值，**只要你能拿到future，几乎可以在任何地方去拿结果**，这是很有用的，在多个模块之间传递数据时可以减少需要考虑的事，而用多个普通std::thread创建的线程，则需要更多的考虑如何拿到结果。

这个例子没有直接使用到promise（其实是有的，只是封装了），为了内容完整，下面看一个使用promise的示例：

```c++
void promiseTest()
{
    std::promise<std::string> p;
    std::future<std::string> f = p.get_future();
    std::thread t1([&p](){ p.set_value("Hello, Guys!"); });
    t1.detach(); //detach，否则会core dump，因为资源没有释放

    //wait会阻塞
    f.wait();
    //这里调用get就不会阻塞了
    std::cout << "message : " << f.get() << std::endl;
}
```

很简单，其实promise就是线程间同步的一种手段，set_value就是往共享状态里设置结果，如果没有设置结果的话，f.wait()或者f.get()会一直阻塞。

#### atomic

最后再简单介绍一下`<atomic>`，这个头文件包含很多原子类型，所谓原子类型，就是具有原子性的类型（有点废话了），对这些类型定义的变量进行操作都是原子操作，即是线程安全的操作，所谓原子操作就是一系列操作不可被其他线程打断。 例如对int类型+1，编译后的指令会是先取数，对数据+1，再写回这3个操作，使用原子类型的话，这3个操作会是连续的，不会被其他线程打断的，所以能保证线程安全。

原子操作的另一个优势就是他是无锁的，上面介绍mutex的有介绍过，mutex在多线程同步的时候会存在一定的性能消耗，主要消耗在锁竞争上，而原子操作因为不存在锁，所以原子类型的性能会高于互斥锁。各位可能会问，不用锁的话，原子操作是怎么保证操作“不会被其他线程打断”的？其实这个是有CPU保证的，CPU的指令集中是有原子指令的，编译器编译后可以生成对应的指令，进而让程序使用CPU提供的原子指令完成原子操作。

`<atomic>`里最重要和最常用的一个类就是atomic类，这是一个模板类，模板参数要求是trivial且可拷贝的，例如int,bool,char，指针以及自定义的满足trivial且可拷贝条件的自定义类型，一般还是使用几个基础类型多一些（具体可参考[C++ atomic with non-trivial type?](https://stackoverflow.com/questions/16355425/c-atomic-with-non-trivial-type)）atomic有两个重要的函数，load和store，分别对应原子读和原子写，也提供了特殊的自增，自简等特殊的操作符重载。同时为了使用方便，头文件中还提供了atomic_bool，atomic_char这种别名给用户使用，具体有哪些，各位可自行查阅文档，不再赘述。下面是一个简单的使用atomic的示例：

```c++
void addCount1(std::atomic<int>& count)
{
    for (int i = 0; i < 1000000; i++) {
        count++;
    }
}

void atomicTest()
{
    std::atomic<int> count(0);
    std::vector<std::thread> threads;
    for (int i = 0; i < 8; i++) {
        threads.push_back(std::thread(addCount1, std::ref(count)));
    }
    
    for (int i = 0; i < threads.size(); i++) {
        threads[i].join();
    }
    
    std::cout << "result Count = " << count.load() << std::endl;
}
```

很简单，不多解释了。

#### thread_local

C++11也把thread_local纳入C++标准里了，thread_local在C++11里是一个关键字，用来修饰变量是所谓的线程本地变量，即多个线程访问该变量，都是在访问线程本地的副本，其原理就类似于一个map，key是线程id，value就是值，当线程访问该变量时，会先用线程id去获取该变量的副本，然后对其进行操作，所以thread_local是天然线程安全的。比较简单，不多介绍了，直接看一个示例：

```c++
void boringFunc3()
{
    thread_local int cnt = 0;
    cnt++;
    std::cout << cnt << std::endl;
}

void boringFunc4()
{
    boringFunc3();
    boringFunc3();
    boringFunc3();
}

void threadLocalTest()
{
    std::vector<std::thread> threads;
    for (int i = 0; i < 8; i++) {
        threads.emplace_back(boringFunc4);
    }
    
    for (int i = 0; i < threads.size(); i++) {
        threads[i].join();
    }
}
```

运行你会发现，每个线程对待cnt都是独立的，不会收到其他线程的影响，对于本例来说，8个线程最终都会答应cnt值是3。

## 小结

本文介绍了多线程相关的几个新头文件，C++11对多线程的支持还是比较丰富的，提供了很多方便使用的API，例如可跨平台的std::thread，线程同步手段mutex，条件变量，异步future/promise, async等。由于篇幅限制，我们没有相信列出各个类提供的所有API，只列出了常用的几个，各位可以自行查阅，有些虽然不常用，但也很有用。

多线程编程能提高程序执行效率，但相比单线程编程来说，需要考虑的事情更多，更复杂，例如需要识别可能存在共享变量并发修改，然后选用适合的机制去保护共享变量，又例如即使使用了互斥量来保护共享变量，又如何防止死锁，C++11提供了多种方法来防止死锁，正文有提到。

最后，借用stackoverflow上的大佬的一句话结尾：Multithreaded programming is hard，respect it!

## 进度



- [ ] [atomic operations library](https://en.cppreference.com/w/cpp/atomic)
- [x] smart pointer
- [ ] `emplace()` and other use of rvalue references throughout all parts of the existing library
  - [x] [std::unique_ptr](https://en.cppreference.com/w/cpp/memory/unique_ptr)
  - [ ] [std::move_iterator](https://en.cppreference.com/w/cpp/iterator/move_iterator)
- [ ] [std::initializer_list](https://en.cppreference.com/w/cpp/utility/initializer_list)
- [ ] [stateful](https://en.cppreference.com/w/cpp/named_req/Allocator#Stateful_and_stateless_allocators) and [scoped](https://en.cppreference.com/w/cpp/memory/scoped_allocator_adaptor) allocators
- [ ] [std::forward_list](https://en.cppreference.com/w/cpp/container/forward_list)
- [ ] [chrono library](https://en.cppreference.com/w/cpp/chrono)
- [ ] [ratio library](https://en.cppreference.com/w/cpp/numeric/ratio)
- [ ] new [algorithms](https://en.cppreference.com/w/cpp/algorithm):
  - [ ] [std::all_of](https://en.cppreference.com/w/cpp/algorithm/all_any_none_of), [std::any_of](https://en.cppreference.com/w/cpp/algorithm/all_any_none_of), [std::none_of](https://en.cppreference.com/w/cpp/algorithm/all_any_none_of),
  - [ ] [std::find_if_not](https://en.cppreference.com/w/cpp/algorithm/find),
  - [ ] [std::copy_if](https://en.cppreference.com/w/cpp/algorithm/copy), [std::copy_n](https://en.cppreference.com/w/cpp/algorithm/copy_n),
  - [ ] [std::move](https://en.cppreference.com/w/cpp/algorithm/move), [std::move_backward](https://en.cppreference.com/w/cpp/algorithm/move_backward),
  - [ ] [std::random_shuffle](https://en.cppreference.com/w/cpp/algorithm/random_shuffle), [std::shuffle](https://en.cppreference.com/w/cpp/algorithm/random_shuffle),
  - [ ] [std::is_partitioned](https://en.cppreference.com/w/cpp/algorithm/is_partitioned), [std::partition_copy](https://en.cppreference.com/w/cpp/algorithm/partition_copy), [std::partition_point](https://en.cppreference.com/w/cpp/algorithm/partition_point),
  - [ ] [std::is_sorted](https://en.cppreference.com/w/cpp/algorithm/is_sorted), [std::is_sorted_until](https://en.cppreference.com/w/cpp/algorithm/is_sorted_until),
  - [ ] [std::is_heap](https://en.cppreference.com/w/cpp/algorithm/is_heap), [std::is_heap_until](https://en.cppreference.com/w/cpp/algorithm/is_heap_until),
  - [ ] [std::minmax](https://en.cppreference.com/w/cpp/algorithm/minmax), [std::minmax_element](https://en.cppreference.com/w/cpp/algorithm/minmax_element),
  - [ ] [std::is_permutation](https://en.cppreference.com/w/cpp/algorithm/is_permutation),
  - [ ] [std::iota](https://en.cppreference.com/w/cpp/algorithm/iota),
  - [ ] [std::uninitialized_copy_n](https://en.cppreference.com/w/cpp/memory/uninitialized_copy_n)
- [ ] [Unicode conversion facets](https://en.cppreference.com/w/cpp/locale#Locale-independent_unicode_conversion_facets)
- [x] [thread library](https://en.cppreference.com/w/cpp/thread)
- [x] [std::function](https://en.cppreference.com/w/cpp/utility/functional/function)
- [x] std::bind
- [ ] [std::exception_ptr](https://en.cppreference.com/w/cpp/error/exception_ptr)
- [ ] [std::error_code](https://en.cppreference.com/w/cpp/error/error_code) and [std::error_condition](https://en.cppreference.com/w/cpp/error/error_condition)
- [ ] [iterator](https://en.cppreference.com/w/cpp/iterator) improvements:
  - [ ] [std::begin](https://en.cppreference.com/w/cpp/iterator/begin)
  - [ ] [std::end](https://en.cppreference.com/w/cpp/iterator/end)
  - [ ] [std::next](https://en.cppreference.com/w/cpp/iterator/next)
  - [ ] [std::prev](https://en.cppreference.com/w/cpp/iterator/prev)
- [ ] Unicode conversion functions